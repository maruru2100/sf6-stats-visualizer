import streamlit as st
import pandas as pd
from sqlalchemy import create_engine, text
from playwright.sync_api import sync_playwright
import datetime
import time
import os
import sys
import pytz
import random
import threading

# --- 1. Áí∞Â¢ÉÂ§âÊï∞„ÅÆ„Éê„É™„Éá„Éº„Ç∑„Éß„É≥ ---
TARGET_ID = os.getenv("TARGET_PLAYER_ID")
DATABASE_URL = os.getenv("DATABASE_URL")

if not TARGET_ID or not DATABASE_URL:
    print("‚ùå „Ç®„É©„Éº: Áí∞Â¢ÉÂ§âÊï∞„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ")
    ENV_ERROR = True
else:
    ENV_ERROR = False

# --- 2. Âü∫Êú¨Ë®≠ÂÆö ---
JST = pytz.timezone('Asia/Tokyo')
engine = create_engine(DATABASE_URL) if not ENV_ERROR else None
COOKIE_PATH = "./auth/local_cookies.json"
FULL_SCREENSHOT_PATH = "./debug_full_screen.png"
LOG_FILE = "scraper.log"

def get_now_jst():
    return datetime.datetime.now(JST)

def write_log(message):
    now = get_now_jst().strftime("%Y-%m-%d %H:%M:%S")
    formatted_msg = f"[{now}] {message}"
    print(formatted_msg)
    try:
        with open(LOG_FILE, "a") as f:
            f.write(formatted_msg + "\n")
    except:
        pass
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = ""
    st.session_state.log_messages += formatted_msg + "\n"

def init_db():
    if ENV_ERROR: return
    with engine.connect() as conn:
        # Êà¶Á∏æ„ÉÜ„Éº„Éñ„É´
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS battle_results (
                id SERIAL PRIMARY KEY, 
                battle_id TEXT UNIQUE, 
                played_at TIMESTAMP, 
                mode TEXT,
                p1_name TEXT, p1_char TEXT, p1_mr INTEGER, p1_control TEXT, p1_result TEXT,
                p2_name TEXT, p2_char TEXT, p2_mr INTEGER, p2_control TEXT, p2_result TEXT
            );
        """))
        # Ë®≠ÂÆö‰øùÂ≠òÁî®„ÉÜ„Éº„Éñ„É´Ôºà„Çπ„Ç±„Ç∏„É•„Éº„É´ÊôÇÈñì„Çí‰øùÂ≠òÔºâ
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS scraper_config (
                key TEXT PRIMARY KEY,
                value TEXT
            );
        """))
        # ÂàùÂõû„ÅÆ„Åø„Éá„Éï„Ç©„É´„ÉàÊôÇÈñì„ÇíÊäïÂÖ•
        conn.execute(text("""
            INSERT INTO scraper_config (key, value) 
            VALUES ('run_times', '09:00,21:00')
            ON CONFLICT (key) DO NOTHING;
        """))
        conn.commit()

# --- 3. Ëß£Êûê„É≠„Ç∏„ÉÉ„ÇØ (Â§âÊõ¥„Å™„Åó) ---
def scrape_sf6(user_code, max_pages=5):
    if not user_code:
        write_log("‚ùå „Ç®„É©„Éº: „É¶„Éº„Ç∂„ÉºID„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
        return False

    target_url = f"https://www.streetfighter.com/6/buckler/ja-jp/profile/{user_code}/battlelog/rank#profile_nav"
    write_log(f"üöÄ „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÈñãÂßã (ID: {user_code}, ÈÅ°„Çä: {max_pages}„Éö„Éº„Ç∏)")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
        context = browser.new_context(
            storage_state=COOKIE_PATH, viewport={'width': 1280, 'height': 1200},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36", locale="ja-JP"
        )
        page = context.new_page()
        all_found_data = []

        try:
            page.goto(target_url, wait_until="networkidle", timeout=60000)
            time.sleep(5)
            # Cookiebot„Éù„ÉÉ„Éó„Ç¢„ÉÉ„ÉóÊéíÈô§
            page.evaluate("""() => {
                const ids = ['#CybotCookiebotDialog', '#CybotCookiebotDialogBodyUnderlay'];
                ids.forEach(id => {
                    const el = document.querySelector(id);
                    if (el) el.remove();
                });
                document.body.style.overflow = 'auto'; 
            }""")
            
            for current_p in range(1, max_pages + 1):
                write_log(f"üìë {current_p}„Éö„Éº„Ç∏ÁõÆ„Çí„Çπ„Ç≠„É£„É≥‰∏≠...")
                time.sleep(2)
                
                page_data = page.evaluate("""
                    () => {
                        const results = [];
                        const items = document.querySelectorAll('li[data-index]');
                        items.forEach((item) => {
                            try {
                                const getPlayerInfo = (sideNum) => {
                                    const pClass = 'battle_data_player' + sideNum;
                                    const parent = item.querySelector(`[class*="${pClass}"]`);
                                    const namePart = item.querySelector(`[class*="battle_data_name_p${sideNum}"]`);
                                    const name = namePart?.innerText.trim() || "Unknown";
                                    const mrText = parent?.querySelector('[class*="battle_data_lp"]')?.innerText || "0";
                                    const mr = parseInt(mrText.replace(/[^0-9]/g, "")) || 0;
                                    const charImg = parent?.querySelector('[class*="battle_data_character"] img');
                                    const charName = charImg?.getAttribute('alt') || "Unknown";
                                    const ctrlImg = parent?.querySelector('[class*="battle_data_control"] img')?.getAttribute('src') || "";
                                    const control = ctrlImg.includes('type0') ? 'Classic' : 'Modern';
                                    const result = item.querySelector(`[class*="battle_data_player_${sideNum}"]`)?.innerText.trim() || "";
                                    return { name, mr, charName, control, result };
                                };
                                const p1 = getPlayerInfo(1);
                                const p2 = getPlayerInfo(2);
                                const dateStr = item.querySelector('[class*="battle_data_date"]')?.innerText.trim();
                                if (dateStr) {
                                    results.push({
                                        id: "rank_" + dateStr.replace(/[^0-9]/g, "") + "_" + p1.name + "_" + p2.name,
                                        date: dateStr, p1, p2
                                    });
                                }
                            } catch (e) {}
                        });
                        return results;
                    }
                """)
                
                if page_data:
                    all_found_data.extend(page_data)
                    write_log(f"‚úÖ {current_p}„Éö„Éº„Ç∏ÁõÆ: {len(page_data)}‰ª∂ÂèñÂæó")

                if current_p < max_pages:
                    next_btn = page.locator("li.next:not(.disabled)").first
                    if next_btn.is_visible():
                        next_btn.click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(random.uniform(3.0, 5.0))
                    else:
                        break

            new_count = 0
            if all_found_data and not ENV_ERROR:
                with engine.connect() as conn:
                    for item in all_found_data:
                        dt = datetime.datetime.strptime(item['date'], "%Y/%m/%d %H:%M")
                        res = conn.execute(text("""
                            INSERT INTO battle_results (
                                battle_id, played_at, mode, p1_name, p1_char, p1_mr, p1_control, p1_result, p2_name, p2_char, p2_mr, p2_control, p2_result
                            )
                            VALUES (:bid, :pat, :mode, :p1n, :p1c, :p1m, :p1ctrl, :p1r, :p2n, :p2c, :p2m, :p2ctrl, :p2r)
                            ON CONFLICT (battle_id) DO NOTHING
                        """), {
                            "bid": item['id'], "pat": dt, "mode": "RankMatch",
                            "p1n": item['p1']['name'], "p1c": item['p1']['charName'], "p1m": item['p1']['mr'], "p1ctrl": item['p1']['control'], "p1r": item['p1']['result'],
                            "p2n": item['p2']['name'], "p2c": item['p2']['charName'], "p2m": item['p2']['mr'], "p2ctrl": item['p2']['control'], "p2r": item['p2']['result']
                        })
                        if res.rowcount > 0: new_count += 1
                    conn.commit()
            
            write_log(f"üèÅ Âá¶ÁêÜÂÆå‰∫Ü„ÄÇÊñ∞Ë¶è‰øùÂ≠ò: {new_count}‰ª∂")
            page.screenshot(path=FULL_SCREENSHOT_PATH)
            return True

        except Exception as e:
            write_log(f"üí• „Ç®„É©„Éº: {str(e)}")
            return False
        finally:
            browser.close()

# --- 4. „Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„ÉâÁõ£Ë¶ñ„Çπ„É¨„ÉÉ„Éâ ---
def background_worker():
    # ÊúÄÂæå„Å´ÂÆüË°å„Åó„Åü„ÄåÊó•‰ªò+ÊôÇÈñì„Äç„ÇíË®òÈå≤„Åô„ÇãÂ§âÊï∞
    last_run = ""
    
    while True:
        if ENV_ERROR: break
        
        now_dt = get_now_jst()
        now_str = now_dt.strftime("%H:%M")
        today_str = now_dt.strftime("%Y-%m-%d")
        current_time_total_minutes = now_dt.hour * 60 + now_dt.minute
        
        # 1. ÊØéÂõûÊúÄÊñ∞„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´„ÇíDB„Åã„ÇâË™≠„ÅøËæº„ÇÄ
        try:
            with engine.connect() as conn:
                res = conn.execute(text("SELECT value FROM scraper_config WHERE key = 'run_times'"))
                row = res.fetchone()
                # „Ç´„É≥„ÉûÂå∫Âàá„Çä„ÅÆÊñáÂ≠óÂàó„Çí„É™„Çπ„Éà„Å´Â§âÊèõ
                raw_times = row[0].split(",") if row else ["09:00", "21:00"]
                
                # „Äå9:00„Äç„Çí„Äå09:00„Äç„Å´Ë£úÊ≠£„Åô„ÇãÂá¶ÁêÜ
                run_times = []
                for t in raw_times:
                    t = t.strip()
                    if len(t) == 4 and ":" in t: # 9:00 „Å™„Å©„ÅÆÂ†¥Âêà
                        t = "0" + t
                    run_times.append(t)
        except:
            # DBÊé•Á∂ö„Ç®„É©„ÉºÁ≠â„ÅÆÂ†¥Âêà„ÅØ„Éá„Éï„Ç©„É´„ÉàÂÄ§„Çí‰ΩøÁî®
            run_times = ["09:00", "21:00"]

        # 2. ÂÆüË°å„Åô„Åπ„ÅçÊôÇÈñì„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
        should_run = False
        matched_time_str = ""

        for t_str in run_times:
            try:
                # Ë®≠ÂÆöÊôÇÂàª(09:00„Å™„Å©)„ÇíÊï∞ÂÄ§ÔºàÂàÜÔºâ„Å´Â§âÊèõ
                h, m = map(int, t_str.split(":"))
                target_total_minutes = h * 60 + m
                
                # Âà§ÂÆöÊù°‰ª∂:
                # ‚ë† ÁèæÂú®ÊôÇÂàª„ÅåË®≠ÂÆöÊôÇÂàª„ÇíÈÅé„Åé„Å¶„ÅÑ„ÇãÔºà„Åæ„Åü„ÅØÂêåÊôÇÔºâ
                # ‚ë° „Åã„Å§„ÄÅÁèæÂú®ÊôÇÂàª„ÅåË®≠ÂÆöÊôÇÂàª„Åã„Çâ1ÊôÇÈñì‰ª•ÂÜÖÔºàÂè§„ÅÑË®≠ÂÆö„ÇíÁÑ°Ë¶ñ„Åô„Çã„Åü„ÇÅÔºâ
                # ‚ë¢ „Åã„Å§„ÄÅ‰ªäÊó•„Åù„ÅÆË®≠ÂÆöÊôÇÂàª„Åß„Åæ„Å†ÂÆüË°å„Åó„Å¶„ÅÑ„Å™„ÅÑ
                is_time_to_go = current_time_total_minutes >= target_total_minutes
                is_not_too_old = current_time_total_minutes < target_total_minutes + 60
                has_not_run_today = last_run != today_str + t_str
                
                if is_time_to_go and is_not_too_old and has_not_run_today:
                    should_run = True
                    matched_time_str = t_str
                    break
            except:
                continue # Â§â„Å™ÂΩ¢Âºè„ÅÆÂÖ•Âäõ„ÅØÁÑ°Ë¶ñ„Åó„Å¶Ê¨°„Å∏

        # 3. ÂÆüË°å
        if should_run:
            write_log(f"‚è∞ ÂÆöÊúüÂ∑°Âõû„Çπ„Ç±„Ç∏„É•„Éº„É´„Å´ÂêàËá¥„Åó„Åæ„Åó„Åü (Ë®≠ÂÆö: {matched_time_str})")
            # TARGET_IDÔºàÁí∞Â¢ÉÂ§âÊï∞„ÅÆÂÄ§Ôºâ„Çí‰ΩøÁî®„Åó„Å¶ÂÆüË°å
            scrape_sf6(TARGET_ID, max_pages=2)
            # ÂÆüË°åÊ∏à„Åø„Çπ„Çø„É≥„Éó„ÇíË®òÈå≤Ôºà‰æã: "2024-05-2109:00"Ôºâ
            last_run = today_str + matched_time_str
            
        # 4. ÂæÖÊ©üÔºà1ÂàÜÈñìÈöîÔºâ
        time.sleep(60)

# --- 5. Streamlit UI ---
st.set_page_config(page_title="SF6 Stats Manager", layout="wide")

if ENV_ERROR:
    st.error("‚ùå Áí∞Â¢ÉÂ§âÊï∞„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
    st.stop()

init_db()

# Â∑¶ÂÅ¥„ÅÆ„Çµ„Ç§„Éâ„Éê„Éº„Åß„Çπ„Ç±„Ç∏„É•„Éº„É´Ë®≠ÂÆö
with st.sidebar:
    st.title("‚öôÔ∏è Ë®≠ÂÆö")
    
    # ÁèæÂú®„ÅÆ„Çπ„Ç±„Ç∏„É•„Éº„É´ÂèñÂæó
    with engine.connect() as conn:
        res = conn.execute(text("SELECT value FROM scraper_config WHERE key = 'run_times'"))
        row = res.fetchone()
        db_times = row[0] if row else "09:00,21:00"

    st.subheader("‚è∞ Ëá™ÂãïÂ∑°Âõû„Çπ„Ç±„Ç∏„É•„Éº„É´")
    new_times = st.text_input("ÂÆüË°åÊôÇÈñì (24hÂΩ¢Âºè„Çí„Ç´„É≥„ÉûÂå∫Âàá„Çä)", value=db_times, help="‰æã: 09:00,15:30,22:00")
    
    if st.button("Ë®≠ÂÆö„Çí‰øùÂ≠ò", use_container_width=True):
        with engine.connect() as conn:
            conn.execute(text("UPDATE scraper_config SET value = :val WHERE key = 'run_times'"), {"val": new_times})
            conn.commit()
        st.success("‚úÖ „Çπ„Ç±„Ç∏„É•„Éº„É´„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü")
        time.sleep(1)
        st.rerun()

    st.divider()
    st.caption("‚Äª„Åì„ÅÆWebÁîªÈù¢„ÇíÈñâ„Åò„Å¶„ÅÑ„Å¶„ÇÇ„ÄÅDocker„ÅåÂãï„ÅÑ„Å¶„ÅÑ„Çå„Å∞Ëá™ÂãïÂèñÂæó„ÅØÁ∂ôÁ∂ö„Åï„Çå„Åæ„Åô„ÄÇ")

# „Éê„ÉÉ„ÇØ„Ç∞„É©„Ç¶„É≥„Éâ„Çπ„É¨„ÉÉ„ÉâËµ∑Âãï
if "worker_thread_started" not in st.session_state:
    if not any(t.name == "BackgroundWorker" for t in threading.enumerate()):
        worker = threading.Thread(target=background_worker, name="BackgroundWorker", daemon=True)
        worker.start()
    st.session_state.worker_thread_started = True

# „É°„Ç§„É≥„Ç≥„É≥„ÉÜ„É≥„ÉÑ
st.title("ü•ä SF6 Êà¶Á∏æÂèéÈõÜ„Ç∑„Çπ„ÉÜ„É†")

col1, col2 = st.columns([1, 1])
with col1:
    st.subheader("ÊâãÂãïÂÆüË°å")
    # TARGET_ID„ÇíÂàùÊúüÂÄ§„Å´„Åó„Å§„Å§„ÄÅÁîªÈù¢„Åß‰∏ÄÊôÇÁöÑ„Å´Â§âÊõ¥„Åó„Å¶ÂÆüË°å„ÇÇÂèØËÉΩ
    current_target = st.text_input("„Çø„Éº„Ç≤„ÉÉ„Éà„É¶„Éº„Ç∂„ÉºID", value=TARGET_ID)
    max_p = st.slider("Â∑°Âõû„Éö„Éº„Ç∏Êï∞", 1, 50, 5)
    
    if st.button("üöÄ ‰ªä„Åô„ÅêÊúÄÊñ∞Êà¶Á∏æ„ÇíÂèñÂæó", use_container_width=True):
        scrape_sf6(current_target, max_pages=max_p)
        st.rerun()

    st.divider()
    st.subheader("„É≠„Ç∞")
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r") as f:
            logs = f.readlines()
            st.text_area("ÂÆüË°åÂ±•Ê≠¥ (ÊúÄÊñ∞50‰ª∂)", value="".join(logs[-50:]), height=300)

with col2:
    st.subheader("ÂâçÂõû„ÅÆÁä∂ÊÖã")
    if os.path.exists(FULL_SCREENSHOT_PATH):
        st.image(FULL_SCREENSHOT_PATH, caption="ÂâçÂõû„ÅÆ„Éñ„É©„Ç¶„Ç∂ÁîªÈù¢")